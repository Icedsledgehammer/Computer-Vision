{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>AIM:</h1>\n",
    "To implement real-time face detection using Haar Cascade classifiers and perform various image processing operations (negative transformation, image enhancement, and background blurring) on captured video frames.\n",
    "\n",
    "<h1>OBJECTIVES:</h1>\n",
    "<ol>\n",
    "<li>Capture live video stream from a webcam</li>\n",
    "<li>Detect faces using Haar Cascade classifier</li>\n",
    "<li>Save extracted frames with face annotations</li>\n",
    "<li>Generate negative (inverted) versions of frames</li>\n",
    "<li>Enhance image quality through denoising, contrast adjustment, and sharpening</li>\n",
    "<li>Implement background blurring while keeping faces in focus</li>\n",
    "</ol>\n",
    "\n",
    "<h1>Computer Vision Theory</h1>\n",
    "\n",
    "## 1. Camera Basics\n",
    "\n",
    "### Digital Camera Operation\n",
    "- Converts light into electronic signals using CMOS/CCD sensors\n",
    "\n",
    "### Frame Resolution\n",
    "- Determined by pixel dimensions (e.g., 640×480 in this implementation)\n",
    "\n",
    "### Frame Rate\n",
    "- Number of frames captured per second (20 fps in this implementation)\n",
    "\n",
    "### Color Space\n",
    "- Uses BGR format in OpenCV (Blue-Green-Red channels)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Haar Cascade Classifiers\n",
    "\n",
    "### Principle\n",
    "- Machine learning-based approach for object detection using Haar-like features\n",
    "\n",
    "### Training\n",
    "- Requires positive (object) and negative (non-object) images\n",
    "\n",
    "### Face Detection\n",
    "- Uses edge and line features to identify facial patterns\n",
    "- Implemented through cascade of simple classifiers (Viola-Jones algorithm)\n",
    "- OpenCV provides pre-trained frontal face detection model\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Image Processing Techniques\n",
    "\n",
    "### Negative Transformation\n",
    "- Pixel inversion using `cv2.bitwise_not()`\n",
    "\n",
    "### Denoising\n",
    "- Non-local Means Denoising preserves details while reducing noise\n",
    "\n",
    "### Contrast Enhancement\n",
    "- CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "- Applied on luminance channel in LAB color space\n",
    "\n",
    "### Sharpening\n",
    "- Convolution with kernel to enhance edges\n",
    "\n",
    "### Background Blurring\n",
    "- Gaussian blur (σ=0, kernel size=51) for background\n",
    "- Face masking using elliptical regions\n",
    "- Alpha blending for smooth transitions\n",
    "\n",
    "---\n",
    "\n",
    "## 4. OpenCV Workflow\n",
    "\n",
    "1. Video capture initialization (`cv2.VideoCapture`)\n",
    "2. Frame-by-frame processing loop\n",
    "3. Grayscale conversion for feature detection\n",
    "4. Haar Cascade face detection (`detectMultiScale`)\n",
    "5. Real-time display and file operations\n",
    "6. Resource cleanup (`release()`, `destroyAllWindows()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install openCV.python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Width 640\n",
      "Frame Height 480\n",
      "Frames saved in the folder: extracted_frames\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#Open default camera\n",
    "cam = cv2.VideoCapture(0)\n",
    "frame_width = int(cam.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cam.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(\"Frame Width\", frame_width)\n",
    "print(\"Frame Height\", frame_height)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('output.mp4', fourcc, 20.0, (frame_width, frame_height))\n",
    "\n",
    "# Load the cascade\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Initialize the video capture\n",
    "cam = cv2.VideoCapture(0)  # Use `0` for webcam or replace with video file path\n",
    "\n",
    "# Create a directory to save the frames\n",
    "output_dir = \"extracted_frames\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "frame_count = 0  # To keep track of the frames\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "    \n",
    "    # Draw rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    \n",
    "    # Save the frame as an image\n",
    "    frame_path = os.path.join(output_dir, f\"frame_{frame_count:04d}.jpg\")  # Save as frame_0001.jpg, frame_0002.jpg, etc.\n",
    "    cv2.imwrite(frame_path, frame)\n",
    "    frame_count += 1\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Camera', frame)\n",
    "    \n",
    "    # Break the loop\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture object\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Frames saved in the folder: {output_dir}\")\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cam.read()\n",
    "#     out.write(frame)\n",
    "#     cv2.imshow('Camera', frame)\n",
    "    \n",
    "#     if cv2.waitKey(1) == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cam.release()\n",
    "# out.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative frames saved in the folder: negative_frames\n"
     ]
    }
   ],
   "source": [
    "# Directories for input frames and output negatives\n",
    "input_dir = \"extracted_frames\"\n",
    "output_dir = \"negative_frames\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through each frame in the input directory\n",
    "for frame_name in os.listdir(input_dir):\n",
    "    frame_path = os.path.join(input_dir, frame_name)\n",
    "    \n",
    "    # Read the frame\n",
    "    frame = cv2.imread(frame_path)\n",
    "    if frame is None:\n",
    "        print(f\"Could not read {frame_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Convert the frame to its negative\n",
    "    negative_frame = cv2.bitwise_not(frame)\n",
    "    \n",
    "    # Save the negative frame\n",
    "    output_path = os.path.join(output_dir, f\"negative_{frame_name}\")\n",
    "    cv2.imwrite(output_path, negative_frame)\n",
    "\n",
    "print(f\"Negative frames saved in the folder: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced frames saved in: enhanced_frames\n"
     ]
    }
   ],
   "source": [
    "# Directories for input frames and output enhanced images\n",
    "input_dir = \"extracted_frames\"\n",
    "output_dir = \"enhanced_frames\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def enhance_image(frame):\n",
    "    \"\"\"Apply enhancement pipeline to a frame\"\"\"\n",
    "    # 1. Denoising\n",
    "    denoised = cv2.fastNlMeansDenoisingColored(frame, None, 10, 10, 7, 21)\n",
    "    \n",
    "    # 2. Contrast Enhancement (CLAHE in LAB color space)\n",
    "    lab = cv2.cvtColor(denoised, cv2.COLOR_BGR2LAB)\n",
    "    l_channel, a_channel, b_channel = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    enhanced_l = clahe.apply(l_channel)\n",
    "    lab_enhanced = cv2.merge((enhanced_l, a_channel, b_channel))\n",
    "    bgr_enhanced = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2BGR)\n",
    "    \n",
    "    # 3. Sharpening\n",
    "    kernel = np.array([[-1, -1, -1],\n",
    "                       [-1,  9, -1],\n",
    "                       [-1, -1, -1]])\n",
    "    sharpened = cv2.filter2D(bgr_enhanced, -1, kernel)\n",
    "    \n",
    "    # 4. Brightness/Contrast Adjustment\n",
    "    alpha = 1.2  # Contrast control (1.0-3.0)\n",
    "    beta = 30    # Brightness control (0-100)\n",
    "    final = cv2.convertScaleAbs(sharpened, alpha=alpha, beta=beta)\n",
    "    \n",
    "    return final\n",
    "\n",
    "# Process all frames in input directory\n",
    "for frame_name in os.listdir(input_dir):\n",
    "    frame_path = os.path.join(input_dir, frame_name)\n",
    "    \n",
    "    # Read frame\n",
    "    frame = cv2.imread(frame_path)\n",
    "    if frame is None:\n",
    "        print(f\"Could not read {frame_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Apply enhancement\n",
    "    enhanced_frame = enhance_image(frame)\n",
    "    \n",
    "    # Save enhanced frame\n",
    "    output_path = os.path.join(output_dir, f\"enhanced_{frame_name}\")\n",
    "    cv2.imwrite(output_path, enhanced_frame)\n",
    "\n",
    "print(f\"Enhanced frames saved in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blurred background frames saved in: blurred_bg_frames\n"
     ]
    }
   ],
   "source": [
    "# Directories for input frames and output blurred background images\n",
    "input_dir = \"extracted_frames\"\n",
    "output_dir = \"blurred_bg_frames\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load face cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "def blur_background(frame):\n",
    "    \"\"\"Apply background blur while keeping faces in focus\"\"\"\n",
    "    # Convert to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "    \n",
    "    # Create a mask for foreground (faces)\n",
    "    mask = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Create elliptical mask for more natural shape\n",
    "        center = (x + w//2, y + h//2)\n",
    "        axes = (int(w*0.6), int(h*0.6))\n",
    "        cv2.ellipse(mask, center, axes, 0, 0, 360, 255, -1)\n",
    "    \n",
    "    # Apply Gaussian blur to entire image\n",
    "    blurred = cv2.GaussianBlur(frame, (51, 51), 0)\n",
    "    \n",
    "    # Combine original and blurred images using the mask\n",
    "    result = np.where(mask[..., np.newaxis], frame, blurred)\n",
    "    \n",
    "    # Optional: Add subtle blur to mask edges for smoother transition\n",
    "    mask_blur = cv2.GaussianBlur(mask, (15, 15), 0)[..., np.newaxis]/255.0\n",
    "    result = (frame * mask_blur + blurred * (1 - mask_blur)).astype(np.uint8)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Process all frames in input directory\n",
    "for frame_name in os.listdir(input_dir):\n",
    "    frame_path = os.path.join(input_dir, frame_name)\n",
    "    \n",
    "    # Read frame\n",
    "    frame = cv2.imread(frame_path)\n",
    "    if frame is None:\n",
    "        print(f\"Could not read {frame_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Apply background blur\n",
    "    blurred_frame = blur_background(frame)\n",
    "    \n",
    "    # Save result\n",
    "    output_path = os.path.join(output_dir, f\"blurred_bg_{frame_name}\")\n",
    "    cv2.imwrite(output_path, blurred_frame)\n",
    "\n",
    "print(f\"Blurred background frames saved in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frames extracted and the video can be found in my github:- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
